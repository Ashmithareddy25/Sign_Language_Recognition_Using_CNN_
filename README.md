# ğŸ§  Sign Language Recognition Using CNN  

This project is an AI-powered **Sign Language Recognition System** built using **Convolutional Neural Networks (CNN)** and **Flask Web Framework**.  
It recognizes American Sign Language (Aâ€“Z) hand gestures from images or live webcam input and converts them into corresponding alphabets â€” even speaking the output aloud using **Google Text-to-Speech (gTTS)**.
![Website Page](output.png)

---

## ğŸ“¸ Features  

- âœ… **Upload-based Prediction** â€“ Upload an image to predict the hand gesture.  
- âœ… **Live Recognition** â€“ Detect signs in real-time using your webcam.  
- âœ… **Speech Output** â€“ Converts the predicted alphabet into speech.  
- âœ… **Simple Web Interface** â€“ Flask-based interactive UI.  
- âœ… **Error Handling** â€“ Validates image formats and missing model files.  

---

## ğŸ—‚ï¸ Project Structure  

```
CODE/
â”‚
â”œâ”€â”€ app.py # Main Flask application
â”œâ”€â”€ Model/
â”‚ â”œâ”€â”€ FinalModel.h5 # Trained CNN model for static predictions
â”‚ â”œâ”€â”€ keras_model.h5 # Model for live recognition (cvzone)
â”‚ â””â”€â”€ labels.txt # Label mapping file
â”œâ”€â”€ images/ # Stores uploaded images
â”œâ”€â”€ templates/ # HTML templates for Flask views
â”‚ â”œâ”€â”€ index.html
â”‚ â”œâ”€â”€ upload.html
â”‚ â”œâ”€â”€ about.html
â”‚ â”œâ”€â”€ services.html
â”‚ â””â”€â”€ contact.html
â”œâ”€â”€ static/ # Optional CSS, JS, or image files
â””â”€â”€ example.mp3 # Auto-generated audio file for predictions
```

---

## âš™ï¸ Installation & Setup  

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/yourusername/Sign_Language_Recognition_Using_CNN.git
cd Sign_Language_Recognition_Using_CNN/CODE
```

### 2ï¸âƒ£ Create and activate a virtual environment
```bash
python -m venv venv
venv\Scripts\activate      # On Windows
# or
source venv/bin/activate   # On Mac/Linux
```
### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
If you donâ€™t have a requirements.txt yet, install manually:
```bash
pip install flask tensorflow keras opencv-python cvzone gTTS numpy
```
## ğŸ§© Model Information  

This project uses two Convolutional Neural Network (CNN) models to recognize American Sign Language (Aâ€“Z) gestures from both uploaded images and real-time webcam input.

| Model File | Purpose |
|-------------|----------|
| **FinalModel.h5** | Classifies uploaded sign language images through the Flask interface |
| **keras_model.h5** | Used for real-time webcam prediction using the CVZone classifier |
| **labels.txt** | Contains the class label mappings for the live recognition model |

Both models are trained on American Sign Language (ASL) datasets covering 26 alphabets (Aâ€“Z), ensuring robust accuracy for both static and dynamic gesture recognition.

## ğŸš€ How to Run  

### â–¶ï¸ Start the Flask App  

From inside the `CODE` directory, run the following command in your terminal or command prompt:

```bash
python app.py
```
You should see an output similar to this:
```bash
ğŸš€ Flask app running...
 * Running on http://127.0.0.1:5000/
```
###ğŸŒ Open in Browser

Once the Flask server starts, open your web browser and navigate to:
```bash
http://127.0.0.1:5000/
```
Youâ€™ll now be able to interact with the Sign Language Recognition Web App â€” upload images or switch to live detection mode to test your trained models in real-time.

## ğŸ’¡ Usage  

### ğŸ”¹ Upload Mode  

1. Go to the **Upload** section in the web app.  
2. Choose a valid image file (`.jpg`, `.png`, `.jpeg`, `.jfif`, `.tif`).  
3. Click **Predict** to classify the uploaded image.  
4. The CNN model predicts the corresponding sign (Aâ€“Z).  
5. The system automatically plays the predicted letter aloud using **gTTS (Google Text-to-Speech)**.  

---

### ğŸ”¹ Live Detection Mode  

1. Navigate to the live detection route:  
```bash
http://127.0.0.1:5000/live
```
2. Allow webcam access when prompted.  
3. The app will start detecting your hand gestures in real time.  
4. Bounding boxes and predicted signs will appear live on the video feed.  
5. Press **ESC** to exit the live recognition window.

## ğŸ—£ï¸ Output Example  

Below are sample outputs demonstrating how the system predicts and speaks sign language alphabets:

| Input Image | Predicted Sign | Audio Output |
|--------------|----------------|---------------|
| âœ‹ | **A** | â€œAâ€ spoken aloud |
| ğŸ¤š | **M** | â€œMâ€ spoken aloud |

---

## âš ï¸ Common Issues & Fixes  

| Issue | Cause | Fix |
|-------|--------|-----|
| `PermissionError: [Errno 13] Permission denied: 'images/'` | Missing filename or invalid save path | âœ… Fixed by using **absolute paths** in `app.py` |
| `Model file not found!` | `FinalModel.h5` missing in `Model/` directory | Place `FinalModel.h5` inside the `Model/` folder |
| Webcam not opening | Camera already in use by another application | Close other apps using webcam and restart Flask |

## ğŸ§  Tech Stack  

| Category | Tools / Technologies |
|-----------|----------------------|
| **Language** | Python |
| **Framework** | Flask |
| **Deep Learning** | TensorFlow / Keras |
| **Computer Vision** | OpenCV, CVZone |
| **Speech** | gTTS (Google Text-to-Speech) |
| **Frontend** | HTML, CSS, Bootstrap |

---

## ğŸ“Š Future Enhancements  

- ğŸ”¹ Add number and word recognition (0â€“9, â€œHelloâ€, â€œThanksâ€, etc.)  
- ğŸ”¹ Implement sentence prediction using sequential models (LSTM).  
- ğŸ”¹ Deploy the application on cloud platforms such as **Streamlit**, **AWS**, or **Render**.  
- ğŸ”¹ Create an Android mobile version using **Flutter** or **React Native**.  

---

## ğŸ‘©â€ğŸ’» Author  

**Ashmitha Reddy Thota**  
ğŸ“ *M.S. in Computer Science â€” Data Science Concentration*  
ğŸ“ *University of North Carolina at Charlotte (UNCC)*  
ğŸ’¼ Passionate about Deep Learning, AI, and Assistive Technologies  









